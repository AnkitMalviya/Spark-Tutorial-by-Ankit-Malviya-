{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Assigment 1 by Ankit Malviya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Download: https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/flight-data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "import pandas as pd\n",
    "#creating spark context\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1)\tRead data with all suitable options like header and others? Also, try to read by different command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flightData2015 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"2015-summary.csv\")\n",
    "flightData2015.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data through pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEST_COUNTRY_NAME</th>\n",
       "      <th>ORIGIN_COUNTRY_NAME</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>Romania</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>United States</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States</td>\n",
       "      <td>India</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DEST_COUNTRY_NAME ORIGIN_COUNTRY_NAME  count\n",
       "0     United States             Romania     15\n",
       "1     United States             Croatia      1\n",
       "2     United States             Ireland    344\n",
       "3             Egypt       United States     15\n",
       "4     United States               India     62"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pandas_data_frame = pd.read_csv('2015-summary.csv')\n",
    "pandas_data_frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2)\tSort the data in descending order of count and store top 20 rows in a new dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "| DEST_COUNTRY_NAME|destination_total|\n",
      "+------------------+-----------------+\n",
      "|     United States|           411352|\n",
      "|            Canada|             8399|\n",
      "|            Mexico|             7140|\n",
      "|    United Kingdom|             2025|\n",
      "|             Japan|             1548|\n",
      "|           Germany|             1468|\n",
      "|Dominican Republic|             1353|\n",
      "|       South Korea|             1048|\n",
      "|       The Bahamas|              955|\n",
      "|            France|              935|\n",
      "|          Colombia|              873|\n",
      "|            Brazil|              853|\n",
      "|       Netherlands|              776|\n",
      "|             China|              772|\n",
      "|           Jamaica|              666|\n",
      "|        Costa Rica|              588|\n",
      "|       El Salvador|              561|\n",
      "|            Panama|              510|\n",
      "|              Cuba|              466|\n",
      "|             Spain|              420|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### implementing it in sql\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")\n",
    "Top20_Sql = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count) as destination_total FROM flight_data_2015 GROUP BY DEST_COUNTRY_NAME ORDER BY sum(count) DESC LIMIT 20 \"\"\")\n",
    "\n",
    "Top20_Sql.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------+\n",
      "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+------------------+-------------------+------+\n",
      "|     United States|      United States|370002|\n",
      "|     United States|             Canada|  8483|\n",
      "|            Canada|      United States|  8399|\n",
      "|     United States|             Mexico|  7187|\n",
      "|            Mexico|      United States|  7140|\n",
      "|    United Kingdom|      United States|  2025|\n",
      "|     United States|     United Kingdom|  1970|\n",
      "|             Japan|      United States|  1548|\n",
      "|     United States|              Japan|  1496|\n",
      "|           Germany|      United States|  1468|\n",
      "|     United States| Dominican Republic|  1420|\n",
      "|Dominican Republic|      United States|  1353|\n",
      "|     United States|            Germany|  1336|\n",
      "|       South Korea|      United States|  1048|\n",
      "|     United States|        The Bahamas|   986|\n",
      "|       The Bahamas|      United States|   955|\n",
      "|     United States|             France|   952|\n",
      "|            France|      United States|   935|\n",
      "|     United States|              China|   920|\n",
      "|          Colombia|      United States|   873|\n",
      "+------------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### implementing it in python\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "sorted_flight_df = flightData2015.sort(desc(\"count\")).limit(20)\n",
    "sorted_flight_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------+\n",
      "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|   count|\n",
      "+------------------+-------------------+--------+\n",
      "|     United States|      United States|370002.0|\n",
      "|     United States|             Canada|  8483.0|\n",
      "|            Canada|      United States|  8399.0|\n",
      "|     United States|             Mexico|  7187.0|\n",
      "|            Mexico|      United States|  7140.0|\n",
      "|    United Kingdom|      United States|  2025.0|\n",
      "|     United States|     United Kingdom|  1970.0|\n",
      "|             Japan|      United States|  1548.0|\n",
      "|     United States|              Japan|  1496.0|\n",
      "|           Germany|      United States|  1468.0|\n",
      "|     United States| Dominican Republic|  1420.0|\n",
      "|Dominican Republic|      United States|  1353.0|\n",
      "|     United States|            Germany|  1336.0|\n",
      "|       South Korea|      United States|  1048.0|\n",
      "|     United States|        The Bahamas|   986.0|\n",
      "|       The Bahamas|      United States|   955.0|\n",
      "|     United States|             France|   952.0|\n",
      "|            France|      United States|   935.0|\n",
      "|     United States|              China|   920.0|\n",
      "|          Colombia|      United States|   873.0|\n",
      "+------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### casting the type of Count\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "Type_casted_df = sorted_flight_df.withColumn(\"count\", flightData2015[\"count\"].cast(DoubleType()))\n",
    "\n",
    "Type_casted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "| DEST_COUNTRY_NAME|destination_total|\n",
      "+------------------+-----------------+\n",
      "|     United States|           411352|\n",
      "|            Canada|             8399|\n",
      "|            Mexico|             7140|\n",
      "|    United Kingdom|             2025|\n",
      "|             Japan|             1548|\n",
      "|           Germany|             1468|\n",
      "|Dominican Republic|             1353|\n",
      "|       South Korea|             1048|\n",
      "|       The Bahamas|              955|\n",
      "|            France|              935|\n",
      "|          Colombia|              873|\n",
      "|            Brazil|              853|\n",
      "|       Netherlands|              776|\n",
      "|             China|              772|\n",
      "|           Jamaica|              666|\n",
      "|        Costa Rica|              588|\n",
      "|       El Salvador|              561|\n",
      "|            Panama|              510|\n",
      "|              Cuba|              466|\n",
      "|             Spain|              420|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### findint total number of flights from a particular destination\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\").sort(desc(\"destination_total\")).limit(20).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3)\tHow can we create a temporary view of the dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")\n",
    "flightData2015.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4)\tRegister dataframe as a table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sqlContext.registerDataFrameAsTable(flightData2015, \"table1\")\n",
    "\n",
    "#sqlContext.dropTempTable(\"table1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5)\tCount of maximum destination country and sort them. Compare spark SQL with Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "| DEST_COUNTRY_NAME|destination_total|\n",
      "+------------------+-----------------+\n",
      "|     United States|           411352|\n",
      "|            Canada|             8399|\n",
      "|            Mexico|             7140|\n",
      "|    United Kingdom|             2025|\n",
      "|             Japan|             1548|\n",
      "|           Germany|             1468|\n",
      "|Dominican Republic|             1353|\n",
      "|       South Korea|             1048|\n",
      "|       The Bahamas|              955|\n",
      "|            France|              935|\n",
      "|          Colombia|              873|\n",
      "|            Brazil|              853|\n",
      "|       Netherlands|              776|\n",
      "|             China|              772|\n",
      "|           Jamaica|              666|\n",
      "|        Costa Rica|              588|\n",
      "|       El Salvador|              561|\n",
      "|            Panama|              510|\n",
      "|              Cuba|              466|\n",
      "|             Spain|              420|\n",
      "+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\").sort(desc(\"destination_total\")).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6)\tFind maximum number of flights to and from any given location? Perform the task using Python and SQL both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for max count from a country\n",
    "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|            Japan|      United States| 1548|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for the flights from US to Japan in python\n",
    "flightData2015.where(flightData2015.DEST_COUNTRY_NAME == 'Japan').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|DEST_COUNTRY_NAME|total_flights_to|\n",
      "+-----------------+----------------+\n",
      "|            Japan|            1548|\n",
      "+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## for the flights from US to Japan in SQL\n",
    "max_dest_df = spark.sql(\"\"\" SELECT DEST_COUNTRY_NAME,sum(count) as total_flights_to FROM flight_data_2015  WHERE DEST_COUNTRY_NAME = \"Japan\" GROUP BY DEST_COUNTRY_NAME \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|            Japan|      United States| 1548|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maximun No. of flighs from Japan to United State in Python\n",
    "flightData2015.where(flightData2015.DEST_COUNTRY_NAME == 'Japan').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|ORIGIN_COUNTRY_NAME|total_flights_frm|\n",
      "+-------------------+-----------------+\n",
      "|              Japan|             1496|\n",
      "+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maximun No. of flighs from Japan to United State in SQL.\n",
    "max_dest_df = spark.sql(\"\"\"SELECT ORIGIN_COUNTRY_NAME,sum(count) as total_flights_frm FROM flight_data_2015 WHERE ORIGIN_COUNTRY_NAME = \"Japan\" GROUP BY ORIGIN_COUNTRY_NAME\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|              Japan| 1496|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maximun No. of flighs from Japan to United State in Python\n",
    "flightData2015.where(flightData2015.ORIGIN_COUNTRY_NAME == 'Japan').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7)\tFind out, top 5 destination countries in the data using both Python and SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 5 destination countries in the data using SQL\n",
    "\n",
    "maxSql = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count) as destination_total FROM flight_data_2015 GROUP BY DEST_COUNTRY_NAME ORDER BY sum(count) DESC LIMIT 5 \"\"\")\n",
    "\n",
    "\n",
    "maxSql.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|         394752.0|\n",
      "|           Canada|           8399.0|\n",
      "|           Mexico|           7140.0|\n",
      "|   United Kingdom|           2025.0|\n",
      "|            Japan|           1548.0|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # top 5 destination countries in the data using Python\n",
    "Type_casted_df.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\").sort(desc(\"destination_total\")).limit(5).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text Data Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'As a group of frogs was traveling through the woods, two of them fell into a deep pit. When the other frogs crowded around the pit and saw how deep it was, they told the two frogs that there was no hope left for them.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDDread=sc.textFile('Text_Analysis.txt')\n",
    "RDDread.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a group of frogs was traveling through the woods, two of them fell into a deep pit\n",
      " When the other frogs crowded around the pit and saw how deep it was, they told the two frogs that there was no hope left for them\n",
      "\n",
      "\n",
      "However, the two frogs decided to ignore what the others were saying and they proceeded to try and jump out of the pit\n",
      " Despite their efforts, the group of frogs at the top of the pit were still saying that they should just give up\n",
      " That they would never make it out\n",
      "\n",
      "\n",
      "Eventually, one of the frogs took heed to what the others were saying and he gave up, falling down to his death\n",
      " The other frog continued to jump as hard as he could\n",
      " Again, the crowd of frogs yelled at him to stop the pain and just die\n",
      "\n",
      "\n",
      "He jumped even harder and finally made it out\n",
      " When he got out, the other frogs said, �Did you not hear us?�\n",
      "\n",
      "The frog explained to them that he was deaf\n",
      " He thought they were encouraging him the entire time\n",
      "\n",
      "\n",
      "Moral of the story: People�s words can have a big effect on other�s lives\n",
      " Think about what you say before it comes out of your mouth\n",
      " It might just be the difference between life and death\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. Convert paragraph into the lines of a document using space.\n",
    "#Reading text file\n",
    "sentences = sc.textFile(\"Text_Analysis.txt\").map(lambda x: \"\".join(x)).flatMap(lambda x: x.split(\".\"))\n",
    "lines = sentences.collect()\n",
    "\n",
    "# Print the lines\n",
    "for sentence in lines:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove the stop words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the stopwords: set([u'all', u'just', u\"don't\", u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'don', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u\"should've\", u\"haven't\", u'do', u'them', u'his', u'very', u\"you've\", u'they', u'not', u'during', u'now', u'him', u'nor', u\"wasn't\", u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u\"won't\", u'where', u\"mustn't\", u\"isn't\", u'few', u'because', u\"you'd\", u'doing', u'some', u'hasn', u\"hasn't\", u'are', u'our', u'ourselves', u'out', u'what', u'for', u\"needn't\", u'below', u're', u'does', u\"shouldn't\", u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u\"mightn't\", u\"doesn't\", u'were', u'here', u'shouldn', u'hers', u\"aren't\", u'by', u'on', u'about', u'couldn', u'of', u\"wouldn't\", u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u\"hadn't\", u'mightn', u\"couldn't\", u'wasn', u'your', u\"you're\", u'from', u'her', u'their', u'aren', u\"it's\", u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u\"didn't\", u'but', u\"that'll\", u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u\"weren't\", u'these', u'up', u'will', u'while', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u\"shan't\", u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u\"you'll\", u'so', u'y', u\"she's\", u'the', u'having', u'once'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'group',\n",
       " u'frogs',\n",
       " u'traveling',\n",
       " u'woods',\n",
       " u'two',\n",
       " u'fell',\n",
       " u'deep',\n",
       " u'pit',\n",
       " u'frogs',\n",
       " u'crowded']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "split_regex = r'\\W+'\n",
    "def simpleTokenize(string):\n",
    "    return filter(lambda strg: len(strg) > 0, re.split(split_regex, string.lower()))\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print 'These are the stopwords: %s' % stop_words\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    return filter(lambda strg: strg not in stop_words, simpleTokenize(string))\n",
    "\n",
    "\n",
    "filtter_sentences = sentences.flatMap(tokenize)\n",
    "\n",
    "filtter_sentences.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3)\tGroup the words in above rdd based on the first 3 characters of each words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'sai', [u'said']), (u'pro', [u'proceeded']), (u'say', [u'saying', u'saying', u'saying', u'say']), (u'saw', [u'saw']), (u'tho', [u'thought'])]\n"
     ]
    }
   ],
   "source": [
    "same_word_grp = filtter_sentences.groupBy(lambda w: w[0:3])\n",
    "print [(k, list(v)) for (k, v) in same_word_grp.take(5)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4)\tCalculate the word frequency in the corpus. // Task1 complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and: 7\n",
      "said,: 1\n",
      "words: 1\n",
      "eventually,: 1\n",
      "frogs: 8\n",
      "into: 1\n",
      "deaf: 1\n",
      "life: 1\n",
      "deep: 2\n",
      "jump: 2\n",
      "as: 3\n",
      "through: 1\n",
      "thought: 1\n",
      "have: 1\n",
      "pit: 4\n",
      "even: 1\n",
      "ignore: 1\n",
      "up,: 1\n",
      "group: 2\n",
      "just: 3\n",
      "for: 1\n",
      "saw: 1\n",
      "top: 1\n",
      "when: 2\n",
      "out,: 1\n",
      "frog: 2\n",
      "finally: 1\n",
      "lives: 1\n",
      "falling: 1\n",
      "was: 3\n",
      "hope: 1\n",
      "them: 3\n",
      "pain: 1\n",
      "again,: 1\n",
      "us?�: 1\n",
      "stop: 1\n",
      "what: 3\n",
      "you: 2\n",
      "hear: 1\n",
      "they: 5\n",
      "not: 1\n",
      "proceeded: 1\n",
      "continued: 1\n",
      "the: 20\n",
      "a: 3\n",
      "down: 1\n",
      "about: 1\n",
      "would: 1\n",
      "woods,: 1\n",
      "might: 1\n",
      "could: 1\n",
      "harder: 1\n",
      "say: 1\n",
      "crowd: 1\n",
      "try: 1\n",
      "�did: 1\n",
      "heed: 1\n",
      "moral: 1\n",
      "however,: 1\n",
      "mouth: 1\n",
      "make: 1\n",
      "comes: 1\n",
      "saying: 3\n",
      "give: 1\n",
      "story:: 1\n",
      "others: 2\n",
      "it: 5\n",
      "one: 1\n",
      "decided: 1\n",
      "traveling: 1\n",
      "left: 1\n",
      "still: 1\n",
      "their: 1\n",
      "out: 4\n",
      "death: 2\n",
      "encouraging: 1\n",
      "no: 1\n",
      "other�s: 1\n",
      "there: 1\n",
      "two: 3\n",
      "should: 1\n",
      "to: 7\n",
      "other: 3\n",
      "between: 1\n",
      "jumped: 1\n",
      "before: 1\n",
      "told: 1\n",
      "be: 1\n",
      "his: 1\n",
      "around: 1\n",
      "big: 1\n",
      "that: 4\n",
      "got: 1\n",
      "never: 1\n",
      "took: 1\n",
      "effect: 1\n",
      "how: 1\n",
      "were: 4\n",
      "fell: 1\n",
      "was,: 1\n",
      "despite: 1\n",
      "explained: 1\n",
      "entire: 1\n",
      "difference: 1\n",
      "him: 2\n",
      "he: 6\n",
      "your: 1\n",
      "on: 1\n",
      "made: 1\n",
      "hard: 1\n",
      "crowded: 1\n",
      "die: 1\n",
      "up: 1\n",
      "can: 1\n",
      "efforts,: 1\n",
      "time: 1\n",
      "people�s: 1\n",
      "of: 9\n",
      "at: 2\n",
      "gave: 1\n",
      "think: 1\n",
      "yelled: 1\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "counts = sentences.flatMap(lambda x: [(w.lower(), 1) for w in x.split()]).reduceByKey(add)\n",
    "\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print \"%s: %i\" % (word, count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform some other transformations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\tCreate sample from complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "33\n",
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'group', u'pit']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_without_rep1 = filtter_sentences.sample(False, 0.3,35)      ##70% sample without Replacement\n",
    "sample_without_rep2 = filtter_sentences.sample(False, 0.3,35)\n",
    "print(filtter_sentences.count())\n",
    "print(sample_without_rep1.count())\n",
    "print(sample_without_rep2.count())\n",
    "\n",
    "sample_without_rep1.take(2)\n",
    "sample_without_rep2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "26\n",
      "26\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample_with_rep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-7346dd64aae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_with_rep1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_with_rep2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msample_with_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_with_rep' is not defined"
     ]
    }
   ],
   "source": [
    "sample_with_rep1 = filtter_sentences.sample(True, 0.3, 35)      ##70% sample with Replacement\n",
    "sample_with_rep2 = filtter_sentences.sample(True, 0.3, 35)\n",
    "print(filtter_sentences.count())\n",
    "print(sample_with_rep1.count())\n",
    "print(sample_with_rep2.count())\n",
    "sample_with_rep.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\tUnion (create 2 sample and try doing union transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "33\n",
      "66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'group', u'pit', u'around', u'saw', u'deep']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_data = sample_without_rep1.union(sample_without_rep2)\n",
    "print(sample_without_rep1.count())\n",
    "print(sample_without_rep2.count())\n",
    "print(union_data.count())\n",
    "union_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "26\n",
      "52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'two', u'fell', u'frogs', u'however', u'two']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_data = sample_with_rep1.union(sample_with_rep2)\n",
    "print(sample_with_rep1.count())\n",
    "print(sample_with_rep2.count())\n",
    "print(union_data.count())\n",
    "union_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### c)\tJoin (Using key and without key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "26\n",
      "72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'm', (u'a', u'a')),\n",
       " (u'm', (u'a', u'i')),\n",
       " (u'm', (u'i', u'a')),\n",
       " (u'm', (u'i', u'i')),\n",
       " (u'e', (u'f', u'f'))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_data = sample_with_rep1.join(sample_with_rep2)\n",
    "print(sample_with_rep1.count())\n",
    "print(sample_with_rep2.count())\n",
    "print(join_data.count())\n",
    "join_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d)\tDistinct in rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filtter_sentences_distinct = filtter_sentences.distinct()\n",
    "len(filtter_sentences_distinct.collect())\n",
    "\n",
    "\n",
    "\n",
    "#filtter_sentences_distinct.take(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
